---
title: "MA 204 Seventh Lab"
author: "YOU AND YOUR PARTNER'S NAMES"
date: "Tuesday, 3/31/15"
output: word_document
---

Today we will get to look at Normal variables!  Hurray!!!!!!!!!!!!!!!!!!!!


####Normal 

Just like with the discrete distributions, R has built in functions that help us learn about the Normal distribution.  

Normal Random variables can be any real number.

Take a moment and try different values in the functions below:

```{r}
dnorm(1, 5, 3)    #returns the pdf evaluated at a particular value for a Normal random variable
pnorm(1, 5,3)    #returns the probability of being <= to a particular value for a normal random variable
rnorm(1, 5, 3)    #generates one or more normal random variables

```

###Question:

for `dnorm` which value is the mu? The first or the second value passed into the function?  What about sigma squared?


#### Baby's weights 

The weight for a newborn baby in the US is distributed Normal, with an average of 7.5 pounds, and the variance is 1.25^2 pounds.    My friend Simone is about to have another baby.  What is the probability this baby is less than 5 pounds?  

I can find out this probability exactly using the command below:
```{r}
pnorm(5, 7.5, 1.25)
```



Or I could simulate this probability:

```{r}
n<-100000
mu<-7.5
sd<-1.25
babies.weight<-rnorm(n, mu, sd)
sum(babies.weight<5)/n  #simulated probability
```

Let's take a look at the distribution of these baby weights we just simulated:

```{r}

hist(babies.weight, main="Simulated Babies Weights")

plot(density(babies.weight), main="Simulated Babies Weights")

```


What if I wanted to convert these babies weights to have a standard normal distribution?

If X~Normal(mu, standard deviation^2)

Z= (X-mu)/(standard deviation)

Z~Normal)(0,1)

```{r}

standardized.weight<- (babies.weight-mu)/sd

hist(standardized.weight, main="Simulated Babies Weights")

plot(density(standardized.weight), main="Simulated Babies Weights")

```

Let's compare that to 100000 standard normal variables:

```{r}

standard.normal<-rnorm(100000, 0,1)

plot(density(standard.normal), main="Standard Normal")

```


Lastly, I can plot the pdf and cdf.  Let's plot these for the standard normal:

```{r}

inputs<-seq(-5,5, by=.1)
pdf<-dnorm(inputs, 0,1)
cdf<-pnorm(inputs, 0, 1)


plot(inputs, pdf, type="l", main="Standard Normal PDF from 5 to -5", col="red", lwd=3)

plot(inputs, cdf, type="l", main="Standard Normal CDF from -5 to 5", col="blue", lwd=3)

```


#####Variance of means

In Statistics we often have a sample mean, and then we want to find what the variance of the mean will be.  

What is the variance of N independent random variables with the same variance (sigma.sq)?
 
Lets simulate this process:


```{r}

rnormalz<-rnorm((100000*5))  #many independent normals with mean 0, variance of 1
rn.matrix<-matrix(rnormalz, ncol=5)  #putting the numbers in a matrix with 5 collumns

```

Lets try the apply function to find the mean in each row:
```{r}

means<-apply(rn.matrix, 1, mean) # takes the median of each row of the rn.matrix 

plot(density(means), col="red", lwd=2, main="mean of 5 N(0,1)")

```

Now lets find the variance of these means:
```{r}

mean(means^2)-mean(means)^2

1/5

```

###Question: What is the variance of N independent random variables with the same variance (sigma.sq)?



####Covariance

Lets look at the Covariance of X~Normal(1,1) and 3X+4.

Note: E(X)=1, E(X^2)=2, E(X^3)=4

COV(X, 3X+4)=E(3X^2 +4X)-E(X)E(3X+4)=?


Lets simulate the covariance:
```{r}
X<-rnorm(100000, 1,1)

plot(density(X), col="orange", lwd=2, main="X~N(1,1)")

lt.X<-3*X+4
plot(density(lt.X), col="green", lwd=2, main="3*X+4, X~N(1,1)")

mean(X*lt.X)-(mean(X)*mean(lt.X))  #Simulated Covariance
cov(X,lt.X)
```
###Exercise: Can you calculate the covariance by hand? Why or why not?

Now lets find the Correlation of X~Normal(1,1) and 3X+4

```{r}

mean(X*lt.X)-(mean(X)*mean(lt.X)) #Simulated Covariance X, 3X+4

mean(X^2)-mean(X)^2 #simulated variance of X

mean(lt.X^2)-mean(lt.X)^2 #simulated variance of X

#correlation

plot(X, lt.X, col="purple", main="X vs. 3*X+4")
cor(X,lt.X)
```

Now Lets look at the Covariance of X~Normal(0,1) and X^2.

Note: E(X)=0, E(X^2)=1, E(X^3)=0

COV(X, X^2)=E(X^3)-E(X)E(X^2)=?

Lets simulate the covariance:
  ```{r}
X<-rnorm(100000, 0,1)

plot(density(X), col="orange", lwd=2, main="X~N(0,1)")

X.sq<-X^2
plot(density(X.sq), col="green", lwd=2, main="X^2, X~N(0,1)")

mean(X*X.sq)-(mean(X)*mean(X.sq))  #Simulated Covariance
cov(X,X.Sq)
```


Now lets find the Correlation of X~Normal(0,1) and X^2

```{r}

mean(X*X.sq)-(mean(X)*mean(X.sq)) #Simulated Covariance X, X^2

mean(X^2)-mean(X)^2 #simulated variance of X

mean(X.sq^2)-mean(X.sq)^2 #simulated variance of X

#correlation

plot(X, X.sq, col="purple", main="X vs. X^2")
cor(X,X.sq)
```





#####Exercise 1 Measurement Error

Scales need to be calibrated so they are not over or under-weighing.  Let's say the amount that a scale is off by in grams is = X~Normal(2, 1). In other words, most items on the scale are shown to be 2 grams more than their actual weight.

3a What is the probability that your scale reports a weight that is less than the actual weight of the object?

```{r}

```


3b Your scale is now going to weigh a gold bar that we know to weigh exactly 10.2 grams.  What is the probability that your scale will give a weight that is more than 11 grams?

```{r}

```


#####Exercise 2 Bivariate Normal AKA joint distribution of Two Normal Variables

In class we learned about how two discrete variables can have joint distributions.  As it turns out two continuous variables can also be jointly distributed.

Play around with the code below to learn about two normal variables (X and Y) that have joint distributions.  Try changing the value of rho (which can go from -1 to 1).  What happens when when rho =-.9, rho=0, when rho=.9?  Describe how the 3D plot of the Joint PDF looks for each of these values of rho. 


```{r}

# Some variable definitions
mu1 <- 0  # mean of X 
mu2 <- 0  # mean of Y 
sig1 <- 1.25	# variance of X 
sig2 <- 1.25	# variance of Y
rho <- .0	# corr(x, y) correlation of X and Y try changing this around

# Some additional variables for x-axis and y-axis 
xm <- -3
xp <- 3
ym <- -3
yp <- 3

X <- seq(xm, xp, length= as.integer((xp + abs(xm)) * 10))  # vector series X
Y <- seq(ym, yp, length= as.integer((yp + abs(ym)) * 10))  # vector series Y

# Core function
bivariate <- function(X,Y){
	term1 <- 1 / (2 * pi * sig1 * sig2 * sqrt(1 - rho^2))
	term2 <- (X - mu1)^2 / sig1^2
	term3 <- -(2 * rho * (X - mu1)*(Y - mu2))/(sig1 * sig2)
	term4 <- (Y - mu2)^2 / sig2^2
	Z <- term2 + term3 + term4
	term5 <- term1 * exp((-Z / (2 *(1 - rho^2))))
	return (term5)
}

# Computes the density values
Z <- outer(X,Y,bivariate)

# Plot
persp(X, Y, Z, main = "Bivariate Normal Distribution",
		sub = bquote(bold(mu[1])==.(mu1)~", "~sigma[1]==.(sig1)~", "~mu[2]==.(mu2)~
		", "~sigma[2]==.(sig2)~", "~rho==.(rho)), zlab="Joint PDF",
		col="lightblue", theta = 30, phi = 45, r = 40, d = 0.1, expand = 0.5,
		ltheta = 90, lphi = 180, shade = 0.3, ticktype = "simple", nticks=5)

```


[INSERT RESPONSE TO EXERCISE 2 HERE]



